---
title: "AI Safety: 5 books that made me interested in the field"
date: "2019-08-19 10:34"
description: "<i><strong style='font-size:26px; color: #B0B0B0'> &ldquo; </strong>The saddest aspect of life right now is that science gathers knowledge faster than society gathers wisdom. <strong style='font-size:26px; color: #B0B0B0'> &rdquo; </i> </strong>– Isaac Asimov"
---


The potential rise of superintelligent AI has gained a lot of traction in recent years and has been deemed to be one of the most [important](https://intelligence.org/why-ai-safety/) discussions of our time.


<p><i><strong style="font-size:26px; color: #B0B0B0"> &ldquo; </strong> As a technologist, I see how AI and the fourth industrial revolution will impact every aspect of people’s lives.  <strong style="font-size:26px; color: #B0B0B0"> &rdquo; </i> </strong></p> – Fei-Fei Li

<p><i><strong style="font-size:26px; color: #B0B0B0"> &ldquo; </strong> The real risk with AI isn't malice but competence. A superintelligent AI will be extremely good at accomplishing its goals, and if those goals aren't aligned with ours, we're in trouble. You're probably not an evil ant-hater who steps on ants out of malice, but if you're in charge of a hydroelectric green energy project and there's an anthill in the region to be flooded, too bad for the ants. Let's not place humanity in the position of those ants.  <strong style="font-size:26px; color: #B0B0B0"> &rdquo; </i> </strong></p>– Stephen Hawking
<p></p>
<p> Here's a non-exhaustive list of non-fiction books I have found informative and relevant to the field of AI Safety. </p>


##  [<div style="text-align: left; color: #4b5b60">1. Superintelligence: Paths, Dangers, Strategies </div>](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies)
<p align="left">
  <img src="https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1400884046l/20527133.jpg" width="200px">
</p>

###<div style="text-align: left; color: #4b5b60"> Why read it? </div>
This 2014 cult classic in the field of AI Safety has been one of the catalysts for the popular concern around building superintelligent AI in the last few years.
<p> </p>

Written in a time when the deep learning revolution had just started to take off, Nick Bostrom's book focuses on the risks of developing machines that are 'vastly smarter' than humans. Beginning with an exploration of 2 possible options: a 'slow takeoff' and a 'fast takeoff', the book rigorously discusses and analyses the risks associated with building superhuman AI.</p>

One of the main challenges he describes is to create a superintelligence that has similar goals and values to us.
This problem of aligning our values is now known as the specification problem or the [AI control problem](https://en.wikipedia.org/wiki/AI_control_problem), and as current research efforts [show](https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1), specifying clear goals and values is a particularly difficult issue that can easily go awry (nice thought experiment [here](https://wiki.lesswrong.com/wiki/Paperclip_maximizer), for fiction examples check out Isaac Asimov's books).
###<div style="text-align: left; color: #4b5b60"> Why is it relevant to AI? </div>
This book gained quite a bit of media coverage due to the likes of [Elon Musk](https://twitter.com/elonmusk/status/495759307346952192?lang=en), [Bill Gates](https://qz.com/698334/bill-gates-says-these-are-the-two-books-we-should-all-read-to-understand-ai/), and [Stuart Russell](https://intelligence.org/2014/07/25/bostrom/), which then helped kickstart AI safety as a serious research field with proper funding, such as Nick Bostrom's own institute at the University of Oxford, The Future of Humanity Institute.
###<div style="text-align: left; color: #4b5b60"> Memorable quote </div>
<i><strong style="font-size:26px; color: #B0B0B0"> &ldquo; </strong>Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.<strong style="font-size:26px; color: #B0B0B0"> &rdquo; </i> </strong>

## [<div style="text-align: left; color: #4b5b60">2. Life 3.0 </div>](https://en.wikipedia.org/wiki/Life_3.0)
<p align="left">
  <img src="https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1522047003l/37857021._SY475_.jpg" width="200px">
</p>

###<div style="text-align: left; color: #4b5b60"> Why read it? </div>
Although its topic is similar to Superintelligence, I found Max Tegmark's 2017 book to be a more balanced view of the future of AI, albeit slightly on the optimistic side.
<p> Despite the fact that both focus on medium to long term societal implications of superintelligent AI, Tegmark tries to look at how we can enforce and maximise a positive outcome, rather than focusing on the potential risks of AI, as described in Bostrom's Superintelligence. </p>

<p> Life 3.0, in Tegmark's view, is a technological stage, where its lifeform designs both its hardware and software (here software is the mechanism through which we process information and make decisions and hardware is the body that stores the information and/or takes action). Life 1.0 is the biological stage, where its hardware and software are entirely a product of evolution, while Life 2.0 is the cultural stage with an evolved hardware and partly designed software (e.g. humans can learn complex skills). </p>

###<div style="text-align: left; color: #4b5b60">  Why is it relevant to AI? </div>
Apart from being a more readable, updated, introduction into the recent developments in AI and AI Safety, Life 3.0 offers a range of scenarios and possibilities of how AI will impact different aspects of humanity that go from the next few years to far into the distant future, which is where his physics background comes most handy.
<p> </p>

I also enjoyed his ventures into more metaphysical topics, such as consciousness or what humanity's purpose might be in the presence of superintelligent AI. Ultimately, I think it hits the right chord; it both terrifies and excites the reader in equal measures about the impact this technology will have.
###<div style="text-align: left; color: #4b5b60">  Memorable quote </div>
<i><strong style="font-size:26px; color: #B0B0B0"> &ldquo; </strong>Without technology, our human extinction is imminent in the cosmic context of tens of billions of years, rendering the entire drama of life in our Universe merely a brief and transient flash of beauty, passion and meaning in a near eternity of meaninglessness experienced by nobody.<strong style="font-size:26px; color: #B0B0B0"> &rdquo; </i> </strong>

## [<div style="text-align: left; color: #4b5b60">3. The structure of scientific revolutions </div>](https://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions)
<p align="left">
  <img src="https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1396422530l/61539.jpg" width="200px">
</p>

###<div style="text-align: left; color: #4b5b60"> Why read it? </div>
Although this book was written in 1962, its take on the progress of science is still very relevant today. Thomas Kuhn introduced the idea that science doesn't develop by accumulating ideas and theories overtime, but rather by periods of slow development, interspersed with periods of revolutionary ideas that partially or entirely replace the established paradigms, a phenomenon which he called 'paradigm shifts'.
<p> </p>
I found this a valuable read in order to better understand how scientific knowledge evolves and how novel ideas initially get suppressed or ignored by the scientific community.
<p> </p>

This particularly amusing [review](https://www.goodreads.com/review/show/516409229?book_show_action=true&from_review_page=1) compares the way scientists hold onto traditional ideas to the way people behave in romantic relationships.

###<div style="text-align: left; color: #4b5b60">  Why is it relevant to AI?  </div>
Given that we are in the midst of an AI revolution at the moment, this book might offer insight into how this revolution happened and how it will proceed. It might help to understand why Deep learning has taken off and why it might take awhile until alternative, more novel approaches replace it.
###<div style="text-align: left; color: #4b5b60">  Memorable quote </div>
<i><strong style="font-size:26px; color: #B0B0B0"> &ldquo; </strong>What man sees depends both upon what he looks at and also upon what his previous visual-conception experience has taught him to see.<strong style="font-size:26px; color: #B0B0B0"> &rdquo; </i> </strong>

## [<div style="text-align: left; color: #4b5b60"> 4. Home Deus: A Brief History of Tomorrow </div>](https://en.wikipedia.org/wiki/Homo_Deus:_A_Brief_History_of_Tomorrow)
<p align="left">
  <img src="https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1468760805l/31138556._SY475_.jpg" width="200px">
</p>

###<div style="text-align: left; color: #4b5b60"> Why read it? </div>
Yuval Noah Harari, a history professor at the Hebrew University in Jerusalem, became famous with his first novel, [Sapiens: A Brief History of Humankind](https://en.wikipedia.org/wiki/Sapiens:_A_Brief_History_of_Humankind). In his 2nd book, Harari focuses on the future of humanity and our increasingly tight relationship with technology, which could result in our evolution from homo sapiens to 'home deus'. In his view, our quest to re-design our minds and bodies to eventually defy death is comparable to a quest to achieve divinity.
<p> </p>

I particularly liked its focus on biotechnology, which, from what I have seen, is rarely part of the recent conversation around AI safety.
###<div style="text-align: left; color: #4b5b60">  Why is it relevant to AI? </div>
I have found Yuval Noah Harari's books to be quite polarising among AI practitioners. Coming from a history/humanitarian background he does seem to hold quite strong speculative opinions that are almost reductive at points, however, they are usually at least great conversation starters.
<p> </p>

Whether you agree with him or not, he does raise important concerns about our identity and our sense of agency in an age of dataism, as highly intelligent AI could end up knowing us better than we know ourselves.
<p> </p>

Are we only going to lose more and more control over our own decisions, becoming merely data points to be fed in ever increasingly intelligent algorithms? Are we making ourselves obsolete in the process of building AGI?
###<div style="text-align: left; color: #4b5b60">  Memorable quote </div>
<i><strong style="font-size:26px; color: #B0B0B0"> &ldquo; </strong>In fact, as time goes by, it becomes easier and easier to replace humans with computer algorithms, not merely because the algorithms are getting smarter, but also because humans are professionalising. Ancient hunter-gatherers mastered a very wide variety of skills in order to survive, which is why it would be immensely difficult to design a robotic hunter-gatherer. Such a robot would have to know how to prepare spear points from flint stones, how to find edible mushrooms in a forest, how to use medicinal herbs to bandage a wound, how to track down a mammoth and how to coordinate a charge with a dozen other hunters. However, over the last few thousand years we humans have been specialising. A taxi driver or a cardiologist specialises in a much narrower niche than a hunter-gatherer, which makes it easier to replace them with AI.
<strong style="font-size:26px; color: #B0B0B0"> &rdquo; </i> </strong>


## [<div style="text-align: left; color: #4b5b60">5. Thinking, Fast and Slow </div>](https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow)
<p align="left">
  <img src="https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1317793965l/11468377.jpg" width="200px">
</p>

###<div style="text-align: left; color: #4b5b60"> Why read it? </div>
In this book, Nobel Prize laureate, Daniel Kahneman, explores the way we think and the multiple kinds of unconscious biases we have when making decisions. His theories are a product of decades of research into behavioural economics and give insight into when we should and shouldn't trust our intuitions.
<p> </p>

Kahneman introduces the "dual-process" model of the brain, namely 2 very different ways of thinking, which he calls System 1 and System 2. System 1 is our fast way of thinking that works by pattern recognition, optimised by millions of years of evolution. System 2, on the other hand, is our slow, rational way of thinking.
<p> </p>

This might be one of the most informative books I have read and it definitely changed the way I look at my own decisions.
###<div style="text-align: left; color: #4b5b60">  Why is it relevant to AI? </div>
When building AI systems or even AGI, we are likely to incorporate our own biases into it. The more we know about them and realise they are there, the more we can build a safe and fair AI.
<p> </p>

If the algorithm seems biased, the issue is likely to be upstream with the data the algorithm was trained on or the model design specified by engineers or researchers building the AI model.
###<div style="text-align: left; color: #4b5b60">  Memorable quote </div>
<i><strong style="font-size:26px; color: #B0B0B0"> &ldquo; </strong>A reliable way to make people believe in falsehoods is frequent repetition, because familiarity is not easily distinguished from truth. Authoritarian institutions and marketers have always known this fact.<strong style="font-size:26px; color: #B0B0B0"> &rdquo; </i> </strong>

## <div style="text-align: left; color: #4b5b60"> Other great resources </div>

- [Deepmind on building safe AI: specification, robustness and assurance](https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1)
- [80000 hours on positively shaping AI](https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/)
- [Open letter on robust and beneficial AI](https://web.archive.org/web/20170311011724/https://futureoflife.org/ai-open-letter)
- [MIRI on the need for AI Safety](https://intelligence.org/why-ai-safety/)
- [OpenAI on AI Safety](https://openai.com/blog/ai-safety-needs-social-scientists/)
- [Nick Bostrom on ethics of AI](https://nickbostrom.com/ethics/artificial-intelligence.pdf)
- [Paper on AFI Safety literature review](https://arxiv.org/pdf/1805.01109.pdf)
- [Paper on Concrete Problems in AI Safety](https://arxiv.org/pdf/1606.06565.pdf)
- [Future of life: A survey of research questions for robust and beneficial AI](https://futureoflife.org/data/documents/research_survey.pdf)


<p> </p>

To end, thought I'd leave this quote here:
<p> </p>

<i><strong style="font-size:26px; color: #B0B0B0"> &ldquo; </strong>The saddest aspect of life right now is that science gathers knowledge faster than society gathers wisdom. <strong style="font-size:26px; color: #B0B0B0"> &rdquo; </i> </strong>– Isaac Asimov

