{"componentChunkName":"component---src-templates-blog-post-js","path":"/AI-Safety-recommended-books/2019-08-19-recommended-nonfiction-books/","webpackCompilationHash":"63c46c68652de35546a3","result":{"data":{"site":{"siteMetadata":{"title":"Blog","author":"Laura Hanu"}},"markdownRemark":{"id":"d7132462-6413-5598-88b2-bcdb45a2d369","excerpt":"The potential rise of superintelligent AI has gained a lot of traction in recent years and has been deemed to be on of the most important discussions of our…","html":"<p>The potential rise of superintelligent AI has gained a lot of traction in recent years and has been deemed to be on of the most important discussions of our time.</p>\n<p><i><strong style=\"font-size:26px; color: #4b5b60\"> \" </strong> As a technologist, I see how AI and the fourth industrial revolution will impact every aspect of people’s lives.  <strong style=\"font-size:26px; color: #4b5b60\"> \" </i> </strong></p> – Fei-Fei Li\n<p><i><strong style=\"font-size:26px; color: #4b5b60\"> \" </strong> The real risk with AI isn't malice but competence. A superintelligent AI will be extremely good at accomplishing its goals, and if those goals aren't aligned with ours, we're in trouble. You're probably not an evil ant-hater who steps on ants out of malice, but if you're in charge of a hydroelectric green energy project and there's an anthill in the region to be flooded, too bad for the ants. Let's not place humanity in the position of those ants.  <strong style=\"font-size:26px; color: #4b5b60\"> \" </i> </strong></p>– Stephen Hawking\n<p></p>\n<p> Here's a non-exhaustive list of nonfiction books I have found informative and relevant to the field of AI. </p>\n<h2><a href=\"https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies\"><div style=\"text-align: left; color: #4b5b60\">1. Superintelligence: Paths, Dangers, Strategies </div></a></h2>\n<p align=\"left\">\n  <img src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1400884046l/20527133.jpg\" width=\"200px\">\n</p>\n<h3><div style=\"text-align: left; color: #4b5b60\"> Why read it? </div></h3>\n<p>This 2012 cult classic started the concern with AI safety. Written in a time when the deep learning revolution had just started to take off, Nick Bostrom’s book focuses on the risks of developing machines that are ‘vastly smarter’ than humans. Beginning with an exploration of 2 possible options: a ‘slow takeoff’ and a ‘fast takeoff’, the book discusses how, if possible, we could ensure the created Superintelligence is friendly. One potential solution would be to create your Superintelligence in such a way that it has similar goals and values to us. This problem of aligning our values is now called the specification problem, and as current research efforts show (for fiction examples see Asimov’s books), specifying clear goals and values is a particularly difficult problem that can easily go wrong (nice thought experiment <a href=\"https://wiki.lesswrong.com/wiki/Paperclip_maximizer\">here</a>).</p>\n<h3><div style=\"text-align: left; color: #4b5b60\"> Why is it relevant to AI? </div></h3>\n<p>This book gained quite a bit of media coverage due to the likes of Elon Musk, Bill Gates and Stuart Russell, which then helped kickstart AI safety as a serious research field with proper funding, such as Nick Bostrom’s own institute at Oxford, The Future of Humanity Institute.</p>\n<h3><div style=\"text-align: left; color: #4b5b60\"> Memorable quote </div></h3>\n<p><i><strong style=\"font-size:26px; color: #4b5b60\"> ” </strong>Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.<strong style=\"font-size:26px; color: #4b5b60\"> ” </i> </strong></p>\n<h2><a href=\"https://en.wikipedia.org/wiki/Life_3.0\"><div style=\"text-align: left; color: #4b5b60\">2. Life 3.0 </div></a></h2>\n<p align=\"left\">\n  <img src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1522047003l/37857021._SY475_.jpg\" width=\"200px\">\n</p>\n<h3><div style=\"text-align: left; color: #4b5b60\"> Why read it? </div></h3>\n<p>Although its topic is similar to Superintelligence, I found Max Tegmark’s book to be a more balanced view of the future of AI, albeit slightly inclined to the rational-optimistic side. Despite the fact that both focus on medium to long term societal implications, Tegmark tries to look at how we can enforce and maximise a positive outcome when it comes to a superintelligent AI, rather than analysing all the potential risks of AI, as described in Bostrom’s Superintelligence. Life 3.0, in Tegmark’s view, is a technological stage, where its lifeform designs both its hardware and software (here software is the mechanism through which we process information and make decisions and hardware is the body that stores the information and/or takes action). Formerly, we had Life 1.0, the biological stage with its hardware and software that were entirely a product of evolution, and Life 2.0, the cultural stage, with an evolved hardware and partly designed software.</p>\n<h3><div style=\"text-align: left; color: #4b5b60\">  Why is it relevant to AI? </div></h3>\n<p>Apart from being a more readable, updated, introduction into the recent developments in AI and AI Safety, Life 3.0 offers a range of scenarios and possibilities of how AI will impact different aspects of humanity that go from the next few years to far into the distant future, where his physics background comes most handy. I also enjoyed his ventures into more metaphysical topics, such as consciousness and what humanity’s purpose might be in the future. Ultimately, I think it hits the right chord of terrifying and exciting the reader in equal measures.</p>\n<h3><div style=\"text-align: left; color: #4b5b60\">  Memorable quote </div></h3>\n<p><i><strong style=\"font-size:26px; color: #4b5b60\"> ” </strong>Without technology, our human extinction is imminent in the cosmic context of tens of billions of years, rendering the entire drama of life in our Universe merely a brief and transient flash of beauty, passion and meaning in a near eternity of meaninglessness experienced by nobody.<strong style=\"font-size:26px; color: #4b5b60\"> ” </i> </strong></p>\n<h2><a href=\"https://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions\"><div style=\"text-align: left; color: #4b5b60\">3. The structure of scientific revolutions </div></a></h2>\n<p align=\"left\">\n  <img src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1396422530l/61539.jpg\" width=\"200px\">\n</p>\n<h3><div style=\"text-align: left; color: #4b5b60\"> Why read it? </div></h3>\n<p>Although this book was written in 1962, its take on the progress of science is still very relevant today. Thomas Kuhn introduced the idea that science doesn’t develop by accumulating ideas and theories overtime, but rather by periods of slow development, interspersed with periods of revolutionary ideas that completely change the established paradigms, a phenomenon which he called ‘paradigm shifts’.</p>\n<h3><div style=\"text-align: left; color: #4b5b60\">  Why is it relevant to AI?  </div></h3>\n<p>Given we are in the midst of an AI revolution at the moment, this book might offer insight into how this revolution happened and how it will progress further on. It might help to understand why Deep learning has taken off and why there is a resistance to alternative, more novel approaches.</p>\n<h3><div style=\"text-align: left; color: #4b5b60\">  Memorable quote </div></h3>\n<p><i><strong style=\"font-size:26px; color: #4b5b60\"> ” </strong>What man sees depends both upon what he looks at and also upon what his previous visual-conception experience has taught him to see.<strong style=\"font-size:26px; color: #4b5b60\"> ” </i> </strong></p>\n<h2><a href=\"https://en.wikipedia.org/wiki/Homo_Deus:_A_Brief_History_of_Tomorrow\"><div style=\"text-align: left; color: #4b5b60\"> 4. Home Deus: A Brief History of Tomorrow </div></a></h2>\n<p align=\"left\">\n  <img src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1468760805l/31138556._SY475_.jpg\" width=\"200px\">\n</p>\n<h3><div style=\"text-align: left; color: #4b5b60\"> Why read it? </div></h3>\n<p>Yuval Noah Harari, a history professor at the Hebrew University in Jerusalem, became famous with his first novel, ‘Sapiens: A Brief History of Humankind’, which I also found a really interesting read, despite it being quite opinionated and occasionally reductive. In his 2nd book, Harari focuses on the future of humanity and our increasingly tight relationship with technology. I particularly liked its focus on biotechnology as well, which, from what I have seen, is rarely part of the recent conversation around AI safety.</p>\n<h3><div style=\"text-align: left; color: #4b5b60\">  Why is it relevant to AI? </div></h3>\n<p>I have found Yuval Noah Harari’s books to be quite polarising among AI practitioners. Sure, coming from a history/humanitarian background he does seem to have some speculative opinions, that may sometimes seem unfounded, however, I think not many people could argue that they are not at least thought provoking or a conversation starter.</p>\n<h3><div style=\"text-align: left; color: #4b5b60\">  Memorable quote </div></h3>\n<p><i><strong style=\"font-size:26px; color: #4b5b60\"> ” </strong>This is the best reason to learn history: not in order to predict the future, but to free yourself of the past and imagine alternative destinies. Of course this is not total freedom – we cannot avoid being shaped by the past. But some freedom is better than none.<strong style=\"font-size:26px; color: #4b5b60\"> ” </i> </strong></p>\n<h2><a href=\"https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow\"><div style=\"text-align: left; color: #4b5b60\">5. Thinking, Fast and Slow </div></a></h2>\n<p align=\"left\">\n  <img src=\"https://i.gr-assets.com/images/S/compressed.photo.goodreads.com/books/1317793965l/11468377.jpg\" width=\"200px\">\n</p>\n<h3><div style=\"text-align: left; color: #4b5b60\"> Why read it? </div></h3>\n<p>In this book, Nobel Prize laureate, Daniel Kahneman, explores the way we think and the multiple kinds of unconscious biases we have when making decisions. His theories are a product of decades of research into behavioural economics and give insight into when we should and shouldn’t trust our intuitions. This might be one of the most informative books I have read and it definitely changed the way I look at my own decisions.</p>\n<h3><div style=\"text-align: left; color: #4b5b60\">  Why is it relevant to AI? </div></h3>\n<p>When building AI systems or even AGI, we are likely to incorporate our own biases into it. The more we know about them and realise they are there, the more we can build a safe and fair AI. If the algorithm seems biased, the issue is likely to be upstream with the data the algorithm was trained on or the model design specified by engineers or researchers building the AI model.</p>\n<h3><div style=\"text-align: left; color: #4b5b60\">  Memorable quote </div></h3>\n<p><i><strong style=\"font-size:26px; color: #4b5b60\"> ” </strong>A reliable way to make people believe in falsehoods is frequent repetition, because familiarity is not easily distinguished from truth. Authoritarian institutions and marketers have always known this fact.<strong style=\"font-size:26px; color: #4b5b60\"> ” </i> </strong></p>\n<h2><div style=\"text-align: left; color: #4b5b60\"> Other great resources </div></h2>\n<ul>\n<li><a href=\"https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1\">Deepmind on building safe AI: specification, robustness and assurance</a></li>\n<li><a href=\"https://80000hours.org/problem-profiles/positively-shaping-artificial-intelligence/\">80000 hours on positively shaping AI</a></li>\n<li><a href=\"https://web.archive.org/web/20170311011724/https://futureoflife.org/ai-open-letter\">Open letter on robust and beneficial AI</a></li>\n<li><a href=\"https://intelligence.org/why-ai-safety/\">MIRI on the need for AI Safety</a></li>\n<li><a href=\"https://openai.com/blog/ai-safety-needs-social-scientists/\">OpenAI on AI Safety</a></li>\n<li><a href=\"https://nickbostrom.com/ethics/artificial-intelligence.pdf\">Nick Bostrom on ethics of AI</a></li>\n<li><a href=\"https://arxiv.org/pdf/1805.01109.pdf\">Paper on AFI Safety literature review</a></li>\n<li><a href=\"https://arxiv.org/pdf/1606.06565.pdf\">Paper on Concrete Problems in AI Safety</a></li>\n<li><a href=\"https://futureoflife.org/data/documents/research_survey.pdf\">Future of life: A survey of research questions for robust and beneficial AI</a></li>\n</ul>","frontmatter":{"title":"5 AI Safety books I recommend","date":"August 19, 2019","description":"This is a list of 5 books that I think have either an interesting take on the future of AI or are relevant for the field of AI."}}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/AI-Safety-recommended-books/2019-08-19-recommended-nonfiction-books/","previous":{"fields":{"slug":"/recommended-fiction-books/2019-08-16-favourite-fiction-books/"},"frontmatter":{"title":"10 Fiction books I recommend"}},"next":null}}}